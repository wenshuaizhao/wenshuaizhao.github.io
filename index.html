<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Wenshuai Zhao</title>

    <meta name="author" content="Wenshuai Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/robot.svg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Wenshuai Zhao
                </p>
                <p>I'm an Electrical Engineering PhD student at <a href="https://rl.aalto.fi">Aalto Robot Learning Lab</a>, Finland. 
                  I have always been fascinated by robots interacting with physical world, other robots and humans. 
                  Motivated by this theme, I started from exploring reinforcement learning algorithms. Recently I am interested in imitation learning for humanoid robots 
                  and learning dynamics model from human videos.
                </p>
                
                <!-- <p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p> -->
                <p style="text-align:center">
                  <a href="mailto:wenshuai.zhao@aalto.fi">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?hl=en&user=cuNOys8AAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
				  <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp; -->
				  <a href="https://www.linkedin.com/in/wenshuai-zhao-76abb4213/">Linkedin</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/wenshuaizhao/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/wszhao/wszhao_circle_1.jpg"><img style="width:70%;max-width:70%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/wszhao/wszhao_circle_1.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <!-- <hr> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I have done several works on mutli-agent reinforcment learning, curriculum learning and model-based reinforcement learning. Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>

 <!-- The following is for the sections of research -->

  <!-- SECTION 1 -->
    
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
              <h3>Multi-agent Reinforcement Learning</h3>
            </td>
          </tr>
        </tbody></table>
      
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='optimappo_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/paper/optimappo/reward_120.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/paper/optimappo/matrix_game.png' width=100% height=100%>
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('optimappo_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('optimappo_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2311.01953.pdf">
          <span class="papertitle">Optimistic Multi-Agent Policy Gradient</span>
        </a>
        <br>
        <strong>Wenshuai Zhao</strong>,
        <a href="https://zhaoyi11.github.io/">Yi Zhao</a>,
        Zhiyuan Li,
        Juho Kannala,
        <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>

        <br>
        <em>ICML</em>, 2024
        <br>
        <a href="https://wenshuaizhao.github.io/optimappo/">website</a>
        /
        <a href="https://github.com/wenshuaizhao/optimappo">code</a>
        /
        <a href="https://arxiv.org/pdf/2311.01953.pdf">arXiv</a>
        <p></p>
        <p>
        In order to overcome the relative overgeneralization problem in multi agent learning, we propose to enable optimism in multi-agent policy gradient methods by reshaping advantages.
        </p>
      </td>
    </tr>


    <tr onmouseout="bpta_stop()" onmouseover="bpta_start()">
      <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bpta_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/paper/bpta/bpta.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/paper/bpta/bpta1.png' width=100% height=100%>
        </div>
        <script type="text/javascript">
          function bpta_start() {
            document.getElementById('bpta_image').style.opacity = "1";
          }

          function bpta_stop() {
            document.getElementById('bpta_image').style.opacity = "0";
          }
          bpta_stop()
        </script>
      </td>
      <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2401.12574.pdf">
          <span class="papertitle">Backpropagation Through Agents</span>
        </a>
        <br>
        Zhiyuan Li,
        <strong>Wenshuai Zhao</strong>,
        Lijun Wu,
        <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>

        <br>
        <em>AAAI</em>, 2024
        <br>
        code
        /
        <a href="https://arxiv.org/pdf/2401.12574.pdf">arXiv</a>
        <p></p>
        <p>
        We propose to backpropogate the gradients through action chains in auto-regressive based MARL methods.
        </p>
      </td>
    </tr>


    <tr onmouseout="bpta_stop()" onmouseover="bpta_start()">
      <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='agentmixer_image'>
            <video  width=100% height=100% muted autoplay loop>
          <source src="images/paper/agentmixer/agentmixer1.png" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/paper/agentmixer/agentmixer1.png' width=100% height=100%>
        </div>
        <script type="text/javascript">
          function bpta_start() {
            document.getElementById('agentmixer_image').style.opacity = "1";
          }

          function bpta_stop() {
            document.getElementById('agentmixer_image').style.opacity = "0";
          }
          bpta_stop()
        </script>
      </td>
      <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2401.08728.pdf">
          <span class="papertitle">AgentMixer: Multi-Agent Correlated Policy Factorization</span>
        </a>
        <br>
        Zhiyuan Li,
        <strong>Wenshuai Zhao</strong>,
        Lijun Wu,
        <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>

        <br>
        <em>arxiv</em>, 2024
        <br>
        code
        /
        <a href="https://arxiv.org/pdf/2401.08728.pdf">arXiv</a>
        <p></p>
        <p>
        We propose multi-agent correlated policy factorization under CTDE, in order to overcome the asymmetric learning failure when naively distill individual policies from a joint policy.
        </p>
      </td>
    </tr>


    <tr onmouseout="currmarl_stop()" onmouseover="currmarl_start()">
      <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
        <div class="one">
          <!-- <div class="two" id='currmarl_image'> -->
            <!-- <video  width=100% height=100% muted autoplay loop> -->
          <!-- <source src="images/paper/currmarl/real.mp4" type="video/mp4"> -->
          <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
          <!-- Your browser does not support the video tag. -->
          <!-- </video></div> -->
          <img src='images/paper/currmarl/protoss.png' width=100% height=100%>
        </div>
        <script type="text/javascript">
          function currmarl_start() {
            document.getElementById('currmarl_image').style.opacity = "1";
          }

          function currmarl_stop() {
            document.getElementById('currmarl_image').style.opacity = "0";
          }
          currmarl_stop()
        </script>
      </td>
      <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2205.10016.pdf">
          <span class="papertitle">Learning Progress Driven Multi-Agent Curriculum</span>
        </a>
        <br>
        <strong>Wenshuai Zhao</strong>,
        Zhiyuan Li,
        <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <!-- <a href="https://github.com/TIERS/partially-observable-marl">code</a> -->
        code
        /
        <a href="https://arxiv.org/pdf/2205.10016.pdf">arXiv</a>
        <p></p>
        <p>
        We show two flaws in existing reward based curriculum learning algorithms when generating number of agents as curriculum in MARL. 
        Instead, we propose a learning progress metric as a new optimization objective which generates curriculum maximizing the learning progress of agents.
        </p>
      </td>
    </tr>
    </tbody></table>

<!-- SECTION 2 -->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
      <h3>Robot Learning</h3>
    </td>
  </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr onmouseout="bmi_stop()" onmouseover="bmi_start()">
    <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='bmi_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/paper/bmi/bmi.mp4" type="video/mp4">
        <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
        Your browser does not support the video tag.
        </video></div>
        <img src='images/paper/bmi/kick.png' width=100% height=100%>
      </div>
      <script type="text/javascript">
        function bmi_start() {
          document.getElementById('bmi_image').style.opacity = "1";
        }

        function bmi_stop() {
          document.getElementById('bmi_image').style.opacity = "0";
        }
        bmi_stop()
      </script>
    </td>
    <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
      <a href="">
        <span class="papertitle">Bi-Level Motion Imitation for Humanoid Robots</span>
      </a>
      <br>
      <strong>Wenshuai Zhao</strong>,
      <a href="https://zhaoyi11.github.io/">Yi Zhao</a>,
      <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>,
      <a href="https://sites.google.com/view/mmuehlebach">Michael Muehlebach</a>
      Michael Muehlebach
      <br>
      <em>CoRL</em>, 2024
      <br>
      code
      /
      arXiv
      <p></p>
      <p>
        We propose a bi-level optimization framework to address the issue of physically infeasible motion data in humanoid imitation learning. 
        The method alternates between optimizing the robot's policy and modifying the reference motions, while using a latent space regularization to preserve the original motion patterns.
      </p>
    </td>
  </tr>

  <tr onmouseout="robustmarl_stop()" onmouseover="robustmarl_start()">
    <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
      <div class="one">
        <div class="two" id='robustmarl_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/paper/robustmarl/real.mp4" type="video/mp4">
        <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
        Your browser does not support the video tag.
        </video></div>
        <img src='images/paper/robustmarl/render.gif' width=100% height=100%>
      </div>
      <script type="text/javascript">
        function robustmarl_start() {
          document.getElementById('robustmarl_image').style.opacity = "1";
        }

        function robustmarl_stop() {
          document.getElementById('robustmarl_image').style.opacity = "0";
        }
        robustmarl_stop()
      </script>
    </td>
    <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2309.14792.pdf">
        <span class="papertitle">Less Is More: Robust Robot Learning via Partially
          Observable Multi-Agent Reinforcement Learning</span>
      </a>
      <br>
      <strong>Wenshuai Zhao*</strong>,
      Eetu-Aleksi Rantala*,
      <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>,
      Jorge Pena Queralta
      <br>
      <em>arXiv</em>, 2023
      <br>
      <a href="https://github.com/TIERS/partially-observable-marl">code</a>
      /
      <a href="https://arxiv.org/pdf/2309.14792.pdf">arXiv</a>
      <p></p>
      <p>
      We show that in many multi agent systems where agents are weakly coupled, partial observation can still enable near-optimal 
      decision making. Moreover, in a mobile robot manipulator, we show partial observation of agents can improve robustness to agent failure.
      </p>
    </td>
  </tr>
  </tbody></table>

    

 <!-- SECTION 3 -->

 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
      <h3>Model-based Reinforcement Learning</h3>
    </td>
  </tr>
</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr onmouseout="currmarl_stop()" onmouseover="currmarl_start()">
    <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
      <div class="one">
        <!-- <div class="two" id='currmarl_image'> -->
          <!-- <video  width=100% height=100% muted autoplay loop> -->
        <!-- <source src="images/paper/currmarl/real.mp4" type="video/mp4"> -->
        <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
        <!-- Your browser does not support the video tag. -->
        <!-- </video></div> -->
        <img src='images/paper/tcrl/dog_run.gif' width=100% height=100%>
      </div>
      <script type="text/javascript">
        function currmarl_start() {
          document.getElementById('currmarl_image').style.opacity = "1";
        }

        function currmarl_stop() {
          document.getElementById('currmarl_image').style.opacity = "0";
        }
        currmarl_stop()
      </script>
    </td>
    <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2306.09466.pdf">
        <span class="papertitle">Simplified Temporal Consistency Reinforcement Learning</span>
      </a>
      <br>
      <a href="https://zhaoyi11.github.io/">Yi Zhao</a>,
      <strong>Wenshuai Zhao</strong>,
      Rinu Boney,
      Juho Kannala
      <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>
      <br>
      <em>ICML</em>, 2023
      <br>
      <!-- <a href="https://github.com/TIERS/partially-observable-marl">code</a> -->
      code
      /
      <a href="https://arxiv.org/pdf/2306.09466.pdf">arXiv</a>
      <p></p>
      <p>
      We propose a simple but effective model-based reinforcement learning algorithm relying only on a latent dynamics model trained
      by latent temporal consistency.
      </p>
    </td>
  </tr>
  </tbody></table>


      <!-- The follwing is for showing the visit map -->

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody>
          <tr>
              <td style="padding:0px">
                  <br>
                  <br>
                  <div>
                    <!-- <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=2-bxAQOsqEuxiVLayk1ZfHX2K8LoP0Uv-LeLJ1fZl4k&cl=ffffff&w=a"></script> -->
                    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=n&d=Xf5__8ayjfq-91Ju307UkI9K7sFkhd8zqWnpnihNRNE&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
                    <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=tt&d=B8VAwe6oif35HWRIEUA3oEdy2w8VKUm5ArDovEOSElc&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
                      <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">-->
                  </div>
              </td>
          </tr>
      </tbody>
    </table>

    

          <!-- </tbody></table> -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>
                  <!-- Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
