<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Wenshuai Zhao</title>

  <meta name="author" content="Wenshuai Zhao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/dog.png" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Wenshuai Zhao
                  </p>
                  <!-- <p>I'm an Electrical Engineering PhD student at <a href="https://rl.aalto.fi">Aalto Robot Learning Lab</a>, Finland. 
                  I have always been fascinated by robots interacting with physical world, other robots and humans. 
                  Motivated by this theme, I started from exploring reinforcement learning algorithms. Recently I am interested in imitation learning for humanoid robots 
                  and learning dynamics model from human videos. -->
                  I'm a PhD student in Electrical Engineering at the <a href="https://rl.aalto.fi">Aalto Robot Learning
                    Lab</a>, Finland.
                  My research focuses on reinforcement learning, imitation learning, and their applications in robotics
                  and general decision-making.
                  I am also interested in robot perception, including 3D scene understanding and physics-based dynamics
                  modeling.
                  </p>
                  <!-- <p style="color: red;">
                  I am now on the job market, looking for post-doc positions in academia or research scientist positions in industry.
                </p> -->

                  <!-- <p>
                  At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>. I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
                </p> -->
                  <p style="text-align: center; margin-bottom: 5px;">
                    Email: wenshuai [dot] zhao [at] aalto [dot] fi
                  </p>
                  <p style="text-align:center; margin-top: 5px;">
                    <!-- <a href="mailto:wenshuai.zhao@aalto.fi">Email</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a
                      href="https://scholar.google.com/citations?hl=en&user=cuNOys8AAAAJ&view_op=list_works&sortby=pubdate">Scholar</a>
                    &nbsp;/&nbsp;
                    <!-- <a href="https://www.threads.net/@jonbarron">Threads</a> &nbsp;/&nbsp; -->
                    <a href="https://www.linkedin.com/in/wenshuai-zhao-74604b81/">Linkedin</a> &nbsp;/&nbsp;
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                    <a href="https://github.com/wenshuaizhao/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/wszhao/wszhao_circle_1.jpg"><img
                      style="width:70%;max-width:70%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/wszhao/wszhao_circle_1.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- <hr> -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
                  <h2>Selected Publications</h2>
                  <p>
                    I have done several projects on imitation learning, mutli-agent reinforcment learning, curriculum
                    learning and model-based reinforcement learning.
                    Representative papers are <span class="highlight">highlighted</span>. Please check my <a
                      href="https://scholar.google.com/citations?hl=en&user=cuNOys8AAAAJ&view_op=list_works&sortby=pubdate">Google
                      Scholar</a> for more details.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- The following is for the sections of research -->

          <!-- SECTION 1 -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
                  <h3>Multi-agent Reinforcement Learning</h3>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="camp_stop()" onmouseover="camp_start()" bgcolor="#ffffd0">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='optimappo_image'>
                      <video width=100% height=100% muted autoplay loop>
                        <source src="images/paper/optimappo/reward_120.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/paper/optimappo/matrix_game.png' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function camp_start() {
                      document.getElementById('optimappo_image').style.opacity = "1";
                    }

                    function camp_stop() {
                      document.getElementById('optimappo_image').style.opacity = "0";
                    }
                    camp_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2311.01953.pdf">
                    <span class="papertitle">Optimistic Multi-Agent Policy Gradient</span>
                  </a>
                  <br>
                  <strong>Wenshuai Zhao</strong>,
                  <a href="https://zhaoyi11.github.io/homepage/">Yi Zhao</a>,
                  <a href="https://lizhyun.github.io/">Zhiyuan Li</a>,
                  Juho Kannala,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>

                  <br>
                  <em>ICML</em>, 2024
                  <br>
                  <a href="https://wenshuaizhao.github.io/optimappo/">website</a>
                  /
                  <a href="https://github.com/wenshuaizhao/optimappo">code</a>
                  /
                  <a href="https://arxiv.org/pdf/2311.01953.pdf">arXiv</a>
                  <p></p>
                  <p>
                    In order to overcome the relative overgeneralization problem in multi agent learning, we propose to
                    enable optimism in multi-agent policy gradient methods by reshaping advantages.
                  </p>
                </td>
              </tr>


              <tr onmouseout="bpta_stop()" onmouseover="bpta_start()">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='bpta_image'>
                      <video width=100% height=100% muted autoplay loop>
                        <source src="images/paper/bpta/bpta.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/paper/bpta/bpta1.png' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function bpta_start() {
                      document.getElementById('bpta_image').style.opacity = "1";
                    }

                    function bpta_stop() {
                      document.getElementById('bpta_image').style.opacity = "0";
                    }
                    bpta_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2401.12574.pdf">
                    <span class="papertitle">Backpropagation Through Agents</span>
                  </a>
                  <br>
                  <a href="https://lizhyun.github.io/">Zhiyuan Li</a>,
                  <strong>Wenshuai Zhao</strong>,
                  Lijun Wu,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>

                  <br>
                  <em>AAAI</em>, 2024
                  <br>
                  code
                  /
                  <a href="https://arxiv.org/pdf/2401.12574.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We propose to backpropogate the gradients through action chains in auto-regressive based MARL
                    methods.
                  </p>
                </td>
              </tr>


              <tr onmouseout="bpta_stop()" onmouseover="bpta_start()">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='agentmixer_image'>
                      <video width=100% height=100% muted autoplay loop>
                        <source src="images/paper/agentmixer/agentmixer1.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/paper/agentmixer/agentmixer1.png' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function bpta_start() {
                      document.getElementById('agentmixer_image').style.opacity = "1";
                    }

                    function bpta_stop() {
                      document.getElementById('agentmixer_image').style.opacity = "0";
                    }
                    bpta_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2401.08728.pdf">
                    <span class="papertitle">AgentMixer: Multi-Agent Correlated Policy Factorization</span>
                  </a>
                  <br>
                  <a href="https://lizhyun.github.io/">Zhiyuan Li</a>,
                  <strong>Wenshuai Zhao</strong>,
                  Lijun Wu,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>

                  <br>
                  <em>AAAI</em>, 2025
                  <br>
                  code
                  /
                  <a href="https://arxiv.org/pdf/2401.08728.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We propose multi-agent correlated policy factorization under CTDE, in order to overcome the
                    asymmetric learning failure when naively distill individual policies from a joint policy.
                  </p>
                </td>
              </tr>


              <tr onmouseout="currmarl_stop()" onmouseover="currmarl_start()" bgcolor="#ffffd0">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='currmarl_image'> -->
                    <!-- <video  width=100% height=100% muted autoplay loop> -->
                    <!-- <source src="images/paper/currmarl/real.mp4" type="video/mp4"> -->
                    <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
                    <!-- Your browser does not support the video tag. -->
                    <!-- </video></div> -->
                    <img src='images/paper/currmarl/protoss.png' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function currmarl_start() {
                      document.getElementById('currmarl_image').style.opacity = "1";
                    }

                    function currmarl_stop() {
                      document.getElementById('currmarl_image').style.opacity = "0";
                    }
                    currmarl_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2205.10016.pdf">
                    <span class="papertitle">Learning Progress Driven Multi-Agent Curriculum</span>
                  </a>
                  <br>
                  <strong>Wenshuai Zhao</strong>,
                  <a href="https://lizhyun.github.io/">Zhiyuan Li</a>,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>
                  <br>
                  <em>arXiv</em>, 2024
                  <br>
                  <!-- <a href="https://github.com/TIERS/partially-observable-marl">code</a> -->
                  code
                  /
                  <a href="https://arxiv.org/pdf/2205.10016.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We show two flaws in existing reward based curriculum learning algorithms when generating number of
                    agents as curriculum in MARL.
                    Instead, we propose a learning progress metric as a new optimization objective which generates
                    curriculum maximizing the learning progress of agents.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- SECTION 2 -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
                  <h3>Robot Learning</h3>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="bmi_stop()" onmouseover="bmi_start()" bgcolor="#ffffd0">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='bmi_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/paper/bmi/bmi.mp4" type="video/mp4">
                        <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/paper/bmi/kick.png' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function bmi_start() {
                      document.getElementById('bmi_image').style.opacity = "1";
                    }

                    function bmi_stop() {
                      document.getElementById('bmi_image').style.opacity = "0";
                    }
                    bmi_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2410.01968">
                    <span class="papertitle">Bi-Level Motion Imitation for Humanoid Robots</span>
                  </a>
                  <br>
                  <strong>Wenshuai Zhao</strong>,
                  <a href="https://zhaoyi11.github.io/homepage/">Yi Zhao</a>,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>,
                  <a href="https://sites.google.com/view/mmuehlebach">Michael Muehlebach</a>
                  <br>
                  <em>CoRL</em>, 2024
                  <br>
                  <a href="https://sites.google.com/view/bmi-corl2024">website</a>
                  /
                  code
                  /
                  <a href="https://arxiv.org/pdf/2410.01968">arXiv</a>
                  <p></p>
                  <p>
                    We propose a bi-level optimization framework to address the issue of physically infeasible motion
                    data in humanoid imitation learning.
                    The method alternates between optimizing the robot's policy and modifying the reference motions,
                    while using a latent space regularization to preserve the original motion patterns.
                  </p>
                </td>
              </tr>

              <tr onmouseout="robustmarl_stop()" onmouseover="robustmarl_start()">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='robustmarl_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/paper/robustmarl/real.mp4" type="video/mp4">
                        <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/paper/robustmarl/render.gif' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function robustmarl_start() {
                      document.getElementById('robustmarl_image').style.opacity = "1";
                    }

                    function robustmarl_stop() {
                      document.getElementById('robustmarl_image').style.opacity = "0";
                    }
                    robustmarl_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2309.14792.pdf">
                    <span class="papertitle">Exploiting Local Observations for Robust Robot Learning</span>
                  </a>
                  <br>
                  <strong>Wenshuai Zhao*</strong>,
                  Eetu-Aleksi Rantala*,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>,
                  Jorge Pena Queralta
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://github.com/TIERS/partially-observable-marl">code</a>
                  /
                  <a href="https://arxiv.org/pdf/2309.14792.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We show that in many multi agent systems where agents are weakly coupled, partial observation can
                    still enable near-optimal
                    decision making. Moreover, in a mobile robot manipulator, we show partial observation of agents can
                    improve robustness to agent failure.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>



          <!-- SECTION 3 -->

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px 0px 20px 20px;width:100%;vertical-align:middle">
                  <h3>Model-based Reinforcement Learning</h3>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="currmarl_stop()" onmouseover="currmarl_start()">
                <td style="padding:0px 0px 20px 20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <!-- <div class="two" id='currmarl_image'> -->
                    <!-- <video  width=100% height=100% muted autoplay loop> -->
                    <!-- <source src="images/paper/currmarl/real.mp4" type="video/mp4"> -->
                    <!-- <source src="images/difsurvey_video.mp4" type="video/mp4"> -->
                    <!-- Your browser does not support the video tag. -->
                    <!-- </video></div> -->
                    <img src='images/paper/tcrl/dog_run.gif' width=100% height=100%>
                  </div>
                  <script type="text/javascript">
                    function currmarl_start() {
                      document.getElementById('currmarl_image').style.opacity = "1";
                    }

                    function currmarl_stop() {
                      document.getElementById('currmarl_image').style.opacity = "0";
                    }
                    currmarl_stop()
                  </script>
                </td>
                <td style="padding:0px 0px 20px 20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2306.09466.pdf">
                    <span class="papertitle">Simplified Temporal Consistency Reinforcement Learning</span>
                  </a>
                  <br>
                  <a href="https://zhaoyi11.github.io/homepage/">Yi Zhao</a>,
                  <strong>Wenshuai Zhao</strong>,
                  Rinu Boney,
                  Juho Kannala,
                  <a href="https://www.aalto.fi/fi/ihmiset/joni-pajarinen">Joni Pajarinen</a>
                  <br>
                  <em>ICML</em>, 2023
                  <br>
                  <a href="https://github.com/zhaoyi11/tcrl">code</a>
                  <!-- code -->
                  /
                  <a href="https://arxiv.org/pdf/2306.09466.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We propose a simple but effective model-based reinforcement learning algorithm relying only on a
                    latent dynamics model trained
                    by latent temporal consistency.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- The follwing is for showing the visit map -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <br>
                  <div>
                    <!-- <script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=2-bxAQOsqEuxiVLayk1ZfHX2K8LoP0Uv-LeLJ1fZl4k&cl=ffffff&w=a"></script> -->
                    <script type='text/javascript' id='clustrmaps'
                      src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=n&d=Xf5__8ayjfq-91Ju307UkI9K7sFkhd8zqWnpnihNRNE&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
                    <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=350&t=tt&d=B8VAwe6oif35HWRIEUA3oEdy2w8VKUm5ArDovEOSElc&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
                    <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">-->
                  </div>
                </td>
              </tr>
            </tbody>
          </table>



          <!-- </tbody></table> -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's
                      website</a>
                    <!-- Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>